{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexledd/So-VITS-SVC-Notebook/blob/main/so-vits-svc_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Lq1uk_WfhG"
      },
      "source": [
        "![alt text](https://repository-images.githubusercontent.com/654490329/ab89913c-c654-48e9-ac2c-ac667cf31155)\n",
        "\n",
        "# **SO-VITS-SVC Colaboratory**\n",
        "\n",
        "#### **üå°Ô∏è Before training**\n",
        "* üíæ This program saves the last 3 generations of models to Google Drive. Since 1 generation of models is >1GB,\n",
        "\n",
        "* üî∫ Make sure your Google Drive have enough storage, 4GB is minimum!\n",
        "\n",
        "* üßë‚Äçüè´ Training requires >10GB VRAM, (T4 should be enough)\n",
        "\n",
        "* ‚úçÔ∏è Inference does not require such a lot of VRAM,\n",
        "\n",
        "* üìÅ If your dataset is  >10 minutes, you need to split it into sections. Split the audio manually or using `Split Tool` below.\n",
        "\n",
        "**üìù Notes: be cautius with your file/folder name, preferably without spaces!**\n",
        "\n",
        "**Also that playing audio directly in Colab can cause runtime to restart. To solve this, download it manually or move it inside /content/drive/MyDrive and play it over GDrive instead**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zzk_dGQIXiiU"
      },
      "outputs": [],
      "source": [
        "#@title NVIDIA SMI (GPU Check)\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dan5obwKROnZ"
      },
      "source": [
        "# Dependencies & Mount Gdrive\n",
        "Restart runtime after everything is installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t_bTO4YJ13Pv"
      },
      "outputs": [],
      "source": [
        "#@title Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ak4zlbFI4OBL"
      },
      "outputs": [],
      "source": [
        "#@title Audio editor dependencies\n",
        "\n",
        "!pip install yt_dlp\n",
        "!pip install ffmpeg\n",
        "!mkdir youtubeaudio\n",
        "!python3 -m pip install -U demucs\n",
        "!python3 -m pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_7o1i5jtN98h"
      },
      "outputs": [],
      "source": [
        "#@title SO-VITS-SVC dependencies\n",
        "\n",
        "!python -m pip install -U pip wheel\n",
        "!pip install pyworld==0.3.2\n",
        "!pip install numpy==1.23.5\n",
        "!pip install -U ipython\n",
        "!pip install -U tensorboard-plugin-profile\n",
        "!pip install -U so-vits-svc-fork\n",
        "!mkdir drive/MyDrive/so-vits-svc-fork\n",
        "#@markdown pip may fail to resolve dependencies and raise ERROR, but it can be ignored.\n",
        "#@markdown You need to restart the runtime after running this cell! (MUST!)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlMZYZIbRKdk"
      },
      "source": [
        "# Training\n",
        "This set is for training an SVC model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZHT28lsARvxP"
      },
      "outputs": [],
      "source": [
        "#@title Make dataset directory\n",
        "!mkdir -p \"dataset_raw\"\n",
        "\n",
        "#!rm -r \"dataset_raw\"\n",
        "#!rm -r \"dataset/44k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JqIVEHklR6fu"
      },
      "outputs": [],
      "source": [
        "#@title Copy your dataset\n",
        "DATASET_NAME = \"\" #@param {type: \"string\"}\n",
        "DATASET_DIR = \"\" #@param {type: \"string\"}\n",
        "\n",
        "DS_TO = f'dataset_raw/{DATASET_NAME}'\n",
        "!mkdir -p {DS_TO}\n",
        "\n",
        "!cp -R {DATASET_DIR}/. -t {DS_TO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IOkrKJoHR_lv"
      },
      "outputs": [],
      "source": [
        "#@title Automatic preprocessing\n",
        "!svc pre-resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VmNJL40gSBGl"
      },
      "outputs": [],
      "source": [
        "#@title Pre-Config for new dataset\n",
        "!svc pre-config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1G0s7Cr_SB2k"
      },
      "outputs": [],
      "source": [
        "#@title Copy configs file\n",
        "!cp configs/44k/config.json drive/MyDrive/so-vits-svc-fork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8oqToxChST9j"
      },
      "outputs": [],
      "source": [
        "#@title  Training Method\n",
        "#@markdown  Here's a brief explanation of each:\n",
        "\n",
        "#@markdown * CREPE (Convolutional Representation for Pitch Estimation): CREPE is a pitch estimation model designed for monophonic audio signals. It uses a deep convolutional neural network (CNN) to estimate the fundamental frequency (pitch) of the input audio.\n",
        "#@markdown * CREPE-Tiny: CREPE-Tiny is a smaller version of the CREPE model. It is a lightweight model with reduced complexity, making it suitable for deployment on resource-constrained devices or environments where computational resources are limited.\n",
        "#@markdown * DIO (Distributed Input/Output): DIO is a fundamental frequency (F0) estimation algorithm. It uses a probabilistic approach called Harvest to estimate the F0 values of audio signals. DIO is particularly effective for polyphonic and noisy audio signals.\n",
        "#@markdown * Parselmouth: Parselmouth is a Python library for the analysis, visualization, and manipulation of speech and music signals. It provides a wide range of functions for extracting various acoustic features, such as pitch, intensity, formants, and spectrograms.\n",
        "#@markdown * Harvest: Harvest is an algorithm used for pitch estimation, implemented as part of the World toolkit. It can estimate the F0 values of monophonic audio signals by analyzing the harmonic structure and periodicity in the signal.\n",
        "F0_METHOD = \"dio\" #@param [\"crepe\", \"crepe-tiny\", \"parselmouth\", \"dio\", \"harvest\"]\n",
        "!svc pre-hubert -fm {F0_METHOD}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EZFaZlUQSnoI"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir drive/MyDrive/so-vits-svc-fork/logs/44k\n",
        "!svc train --model-path drive/MyDrive/so-vits-svc-fork/logs/44k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AeEsZTZ4S4fj"
      },
      "outputs": [],
      "source": [
        "#@title Training Cluster Model\n",
        "!svc train-cluster --output-path drive/MyDrive/so-vits-svc-fork/logs/44k/kmeans.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_99k5PV_TSi9"
      },
      "source": [
        "# Inference\n",
        "This set is for using the SO-VITS-SVC model for conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "vlnt26CKTWP9"
      },
      "outputs": [],
      "source": [
        "#@title **INFERENCE**\n",
        "#@markdown #INFERING USING PRE/TRAINED SVC MODEL\n",
        "#@markdown * remove **\".wav\"** on AUDIO\n",
        "from IPython.display import Audio\n",
        "\n",
        "AUDIO = \"\" #@param {type:\"string\"}\n",
        "MODEL = \"\" #@param {type:\"string\"}\n",
        "CONFIG = \"\" #@param {type:\"string\"}\n",
        "#@markdown * Change according to your model's voice pitch. 12 = 1 Octave | -12 = -1 Octave.\n",
        "#@markdown * Higher pitch audio to Lower pitch Model usually use -12 to -24; Vice Versa\n",
        "PITCH = -12 #@param {type:\"integer\"}\n",
        "#@markdown * Options, or leave it by default\n",
        "Auto_Predict = False #@param {type:\"boolean\"}\n",
        "Pitch_Bypass = False #@param {type:\"boolean\"}\n",
        "DisplayAudio_Infer = False #@param {type:\"boolean\"}\n",
        "\n",
        "def Auto_PredictFalse():\n",
        "  if Pitch_Bypass:\n",
        "    !svc infer {AUDIO}.wav -c {CONFIG} -m {MODEL} -na\n",
        "  else:\n",
        "    !svc infer {AUDIO}.wav -c {CONFIG} -m {MODEL} -na -t {PITCH}\n",
        "\n",
        "def Auto_PredictTrue():\n",
        "  if Pitch_Bypass:\n",
        "    !svc infer {AUDIO}.wav -c {CONFIG} -m {MODEL}\n",
        "  else:\n",
        "    !svc infer {AUDIO}.wav -c {CONFIG} -m {MODEL} -t {PITCH}\n",
        "\n",
        "if Auto_Predict:\n",
        "    Auto_PredictTrue()\n",
        "else:\n",
        "    Auto_PredictFalse()\n",
        "\n",
        "#@markdown Displaying audio can restart the runtime sometimes\n",
        "if DisplayAudio_Infer :\n",
        "  display(Audio(f\"{AUDIO}.out.wav\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Zlj0fa-RNd"
      },
      "source": [
        "# Downloader\n",
        "This cell is for downloading from the internet; url must be direct to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7Be8zOQpUAVY"
      },
      "outputs": [],
      "source": [
        "#@title Downloader\n",
        "#@markdown The default downloads folder is in \"/content/downloaded\"\n",
        "file_url = \"\" #@param {type:\"string\"}\n",
        "file_url2 = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!mkdir downloaded\n",
        "!wget -N {file_url} -P downloaded/\n",
        "!wget -N {file_url2} -P downloaded/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iQG8AqbqPa3c"
      },
      "outputs": [],
      "source": [
        "#@title YouTube Audio Downloader (WAV Output)\n",
        "from __future__ import unicode_literals\n",
        "import yt_dlp\n",
        "import ffmpeg\n",
        "import sys\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "#    'outtmpl': 'output.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }],\n",
        "    \"outtmpl\": 'youtubeaudio/audio',  # this is where you can edit how you'd like the filenames to be formatted\n",
        "}\n",
        "def download_from_url(url):\n",
        "    ydl.download([url])\n",
        "    # stream = ffmpeg.input('output.m4a')\n",
        "    # stream = ffmpeg.output(stream, 'output.wav')\n",
        "\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "      url = \"\" #@param {type:\"string\"}\n",
        "      download_from_url(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G7IPEzmQEp0e"
      },
      "outputs": [],
      "source": [
        "#@title Unzip Tool\n",
        "ZIP_PATH = \"\" #@param {type:\"string\"}\n",
        "FOLDER_NAME = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!unzip {ZIP_PATH} -d {FOLDER_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_-DIk1RgTL"
      },
      "source": [
        "# Audio Editor\n",
        "This set is for audio editing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split Tool\n",
        "from scipy.io import wavfile\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Utility functions\n",
        "\n",
        "def GetTime(video_seconds):\n",
        "\n",
        "    if (video_seconds < 0) :\n",
        "        return 00\n",
        "\n",
        "    else:\n",
        "        sec = timedelta(seconds=float(video_seconds))\n",
        "        d = datetime(1,1,1) + sec\n",
        "\n",
        "        instant = str(d.hour).zfill(2) + ':' + str(d.minute).zfill(2) + ':' + str(d.second).zfill(2) + str('.001')\n",
        "\n",
        "        return instant\n",
        "\n",
        "def GetTotalTime(video_seconds):\n",
        "\n",
        "    sec = timedelta(seconds=float(video_seconds))\n",
        "    d = datetime(1,1,1) + sec\n",
        "    delta = str(d.hour) + ':' + str(d.minute) + \":\" + str(d.second)\n",
        "\n",
        "    return delta\n",
        "\n",
        "def windows(signal, window_size, step_size):\n",
        "    if type(window_size) is not int:\n",
        "        raise AttributeError(\"Window size must be an integer.\")\n",
        "    if type(step_size) is not int:\n",
        "        raise AttributeError(\"Step size must be an integer.\")\n",
        "    for i_start in range(0, len(signal), step_size):\n",
        "        i_end = i_start + window_size\n",
        "        if i_end >= len(signal):\n",
        "            break\n",
        "        yield signal[i_start:i_end]\n",
        "\n",
        "def energy(samples):\n",
        "    return np.sum(np.power(samples, 2.)) / float(len(samples))\n",
        "\n",
        "def rising_edges(binary_signal):\n",
        "    previous_value = 0\n",
        "    index = 0\n",
        "    for x in binary_signal:\n",
        "        if x and not previous_value:\n",
        "            yield index\n",
        "        previous_value = x\n",
        "        index += 1\n",
        "\n",
        "# Change the parameters here\n",
        "split_input = \"\" #@param {type:\"string\"}\n",
        "split_output = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown The minimum length of silence at which a split may occur [seconds]. Defaults to 3 seconds.\n",
        "min_silence_length = 0.6 #@param {type:\"number\"}\n",
        "#@markdown The energy level (between 0.0 and 1.0) below which the signal is regarded as silent.\n",
        "silence_threshold = 1e-4 #@param {type:\"number\"}\n",
        "#@markdown The amount of time to step forward in the input file after calculating energy. Smaller value = slower, but more accurate silence detection. Larger value = faster, but might miss some split opportunities. Defaults to **(min-silence-length / 10.)**.\n",
        "step_duration = 0.03/10 #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Or leave it as my parameters= *0.6 ; 1e-4 ; 0.003/10*\n",
        "\n",
        "input_filename = split_input\n",
        "window_duration = min_silence_length\n",
        "if step_duration is None:\n",
        "    step_duration = window_duration / 10.\n",
        "else:\n",
        "    step_duration = step_duration\n",
        "\n",
        "output_filename_prefix = os.path.splitext(os.path.basename(input_filename))[0]\n",
        "dry_run = False\n",
        "\n",
        "print(\"Splitting {} where energy is below {}% for longer than {}s.\".format(\n",
        "    input_filename,\n",
        "    silence_threshold * 100.,\n",
        "    window_duration\n",
        "    )\n",
        ")\n",
        "\n",
        "# Read and split the file\n",
        "\n",
        "sample_rate, samples = input_data=wavfile.read(filename=input_filename, mmap=True)\n",
        "\n",
        "max_amplitude = np.iinfo(samples.dtype).max\n",
        "print(max_amplitude)\n",
        "\n",
        "max_energy = energy([max_amplitude])\n",
        "print(max_energy)\n",
        "\n",
        "window_size = int(window_duration * sample_rate)\n",
        "step_size = int(step_duration * sample_rate)\n",
        "\n",
        "signal_windows = windows(\n",
        "    signal=samples,\n",
        "    window_size=window_size,\n",
        "    step_size=step_size\n",
        ")\n",
        "\n",
        "window_energy = (energy(w) / max_energy for w in tqdm(\n",
        "    signal_windows,\n",
        "    total=int(len(samples) / float(step_size))\n",
        "))\n",
        "\n",
        "window_silence = (e > silence_threshold for e in window_energy)\n",
        "\n",
        "cut_times = (r * step_duration for r in rising_edges(window_silence))\n",
        "\n",
        "# This is the step that takes long, since we force the generators to run.\n",
        "print(\"Finding silences...\")\n",
        "cut_samples = [int(t * sample_rate) for t in cut_times]\n",
        "cut_samples.append(-1)\n",
        "\n",
        "cut_ranges = [(i, cut_samples[i], cut_samples[i+1]) for i in range(len(cut_samples) - 1)]\n",
        "\n",
        "video_sub = {str(i) : [str(GetTime(((cut_samples[i])/sample_rate))),\n",
        "                       str(GetTime(((cut_samples[i+1])/sample_rate)))]\n",
        "             for i in range(len(cut_samples) - 1)}\n",
        "\n",
        "for i, start, stop in tqdm(cut_ranges):\n",
        "    output_file_path = \"{}_{:03d}.wav\".format(\n",
        "        os.path.join(split_output, output_filename_prefix),\n",
        "        i\n",
        "    )\n",
        "    if not dry_run:\n",
        "        print(\"Writing file {}\".format(output_file_path))\n",
        "        wavfile.write(\n",
        "            filename=output_file_path,\n",
        "            rate=sample_rate,\n",
        "            data=samples[start:stop]\n",
        "        )\n",
        "    else:\n",
        "        print(\"Not writing file {}\".format(output_file_path))\n",
        "\n",
        "with open (split_output+'\\\\'+output_filename_prefix+'.json', 'w') as output:\n",
        "    json.dump(video_sub, output)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SmuOMpYodBWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z_UroiCWUqGD"
      },
      "outputs": [],
      "source": [
        "#@title Convert to Waveform (.WAV)\n",
        "#@markdown remove the file extension (.mp3;m4a) in input section. default output is in \"/content/converted\"\n",
        "FFMPEG_INPUT = \"\" #@param {type:\"string\"}\n",
        "FILE_EXT = \"\" #@param {type:\"string\"}\n",
        "OUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!mkdir converted\n",
        "!ffmpeg -i {FFMPEG_INPUT}.{FILE_EXT} -acodec pcm_s16le /content/converted/{OUT}.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3_H8nINjQAEn"
      },
      "outputs": [],
      "source": [
        "#@title Demuxer (Seperate Vocal and Background)\n",
        "import subprocess\n",
        "AUDIO_INPUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "command = f\"demucs --two-stems=vocals {AUDIO_INPUT}\"\n",
        "result = subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
        "print(result.stdout.decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-MVhrL0tE4yp"
      },
      "outputs": [],
      "source": [
        "#@title Analyzing Audio Volume\n",
        "ANLZ_INPUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!ffmpeg -i {ANLZ_INPUT} -filter:a volumedetect -f null /dev/null!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TuQCJbLwFYeZ"
      },
      "outputs": [],
      "source": [
        "#@title Volume Manipulation\n",
        "VM_INPUT = \"\" #@param {type:\"string\"}\n",
        "#@markdown Value can be in \"1.5\" (150% Increase) or in \"10dB\" (10dB Increase)\n",
        "VM_VALUE = \"\" #@param {type:\"string\"}\n",
        "#@markdown Output filename; In /content/volume_changed\n",
        "VM_OUTPUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!mkdir volume_changed\n",
        "!ffmpeg -i {VM_INPUT} -filter:a \"volume={VM_VALUE}\" -c:a pcm_s16le /content/volume_changed/{VM_OUTPUT}.volume.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_AL_8MBPA7RK"
      },
      "outputs": [],
      "source": [
        "#@title Audio Normalization\n",
        "#@markdown * Audio Normalization input; this cell will also convert audio file to waveform.\n",
        "AN_INPUT = \"\" #@param {type:\"string\"}\n",
        "#@markdown * Target loudness; type just the value in dB (ex. \"-6\")\n",
        "TARGET_LDNS = \"-6\" #@param {type:\"string\"}\n",
        "#@markdown * The default Loudness Range is 11dB\n",
        "RANGE_LDNS = \"11\" #@param {type:\"string\"}\n",
        "#@markdown * The default value is -1.5dB\n",
        "TRUE_PEAK = \"-1.5\" #@param {type:\"string\"}\n",
        "#@markdown * Output filename; in /content/normalized\n",
        "AN_OUTPUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!mkdir normalized\n",
        "!ffmpeg -i {AN_INPUT} -af loudnorm=I={TARGET_LDNS}:LRA={RANGE_LDNS}:TP={TRUE_PEAK} -c:a pcm_s16le /content/normalized/{AN_OUTPUT}.normalized.wav\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9BhLtRDiT1-r"
      },
      "outputs": [],
      "source": [
        "#@title Combine\n",
        "from pydub import AudioSegment\n",
        "!mkdir combined\n",
        "\n",
        "AUDIO_01 = \"\" #@param {type:\"string\"}\n",
        "AUDIO_02 = \"\" #@param {type:\"string\"}\n",
        "DisplayAudio_Combined = False #@param {type:\"boolean\"}\n",
        "\n",
        "sound1 = AudioSegment.from_file(AUDIO_01)\n",
        "sound2 = AudioSegment.from_file(AUDIO_02)\n",
        "\n",
        "combined = sound1.overlay(sound2)\n",
        "\n",
        "combined.export(\"/content/combined/audio.combined.wav\", format='wav')\n",
        "\n",
        "def DisplayAudioResult():\n",
        "    display(Audio(f\"/content/combined/audio.combined.wav\"))\n",
        "\n",
        "if DisplayAudio_Combined :\n",
        "  DisplayAudioResult()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Audio Recording\n",
        "REC_NAME = \"Audio.wav\" #@param {type:\"string\"}\n",
        "REC_OUT = \"/content/\" #@param {type:\"string\"}\n",
        "REC_COMB = os.path.join(REC_OUT, REC_NAME)\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "  };\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.innerText = \"Recording... Press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording...\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  recordButton.onclick = ()=>{\n",
        "    toggleRecording();\n",
        "\n",
        "    sleep(2000).then(() => {\n",
        "      resolve(base64data.toString());\n",
        "    });\n",
        "  };\n",
        "});\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import ffmpeg\n",
        "\n",
        "def get_audio():\n",
        "    display(HTML(AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        [\"ffmpeg\", \"-i\", \"pipe:0\", \"-f\", \"wav\", \"pipe:1\"],\n",
        "        stdin=subprocess.PIPE,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "    )\n",
        "    output, err = process.communicate(input=binary)\n",
        "\n",
        "    riff_chunk_size = len(output) - 8\n",
        "    # Break up the chunk size into four bytes, held in b.\n",
        "    q = riff_chunk_size\n",
        "    b = []\n",
        "    for i in range(4):\n",
        "        q, r = divmod(q, 256)\n",
        "        b.append(r)\n",
        "\n",
        "    # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "    riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "    sr, audio = wavfile.read(io.BytesIO(riff))\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "audio, sr = get_audio()\n",
        "wavfile.write(REC_COMB, sr, audio)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QIqYUfo2zz4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Dan5obwKROnZ",
        "x4Zlj0fa-RNd",
        "lH_-DIk1RgTL",
        "BlMZYZIbRKdk",
        "_99k5PV_TSi9"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyP3dAGkPf4Ys3dmetx6O948",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}